{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to evaluate SAEs when used in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use HookedTransformer hooks to replace the specified component output with it's SAE counterpart\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from transformer_lens import HookedTransformer, utils\n",
    "import torch\n",
    "import nnsight\n",
    "device = 'cuda'\n",
    "from tasks.ioi.IOITask import IOITask\n",
    "from tasks.facts.SportsTask import SportsTask\n",
    "from tasks.owt.OWTTask import OWTTask\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-70m-deduped into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    'EleutherAI/pythia-70m-deduped',\n",
    "    # 'EleutherAI/pythia-1.4b-deduped',\n",
    "    # 'EleutherAI/pythia-2.8b-deduped',\n",
    "    device=device\n",
    ")\n",
    "\n",
    "model.set_use_hook_mlp_in(True)\n",
    "tokenizer = model.tokenizer\n",
    "batch_size=500\n",
    "\n",
    "ioi_task = IOITask(batch_size=batch_size, tokenizer=tokenizer, device=device, handle_multitoken_labels=True, num_data=1000)\n",
    "sports_task = SportsTask(batch_size=batch_size, tokenizer=tokenizer, device=device)\n",
    "owt_task = OWTTask(batch_size=batch_size, tokenizer=tokenizer, device=device, ctx_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dictionary_learning.dictionary import AutoEncoder\n",
    "\n",
    "layer = 1\n",
    "hidden_layer = False # if True, use the hidden layer, else use the output layer\n",
    "submodule = model.blocks[layer].mlp # layer 1 MLP\n",
    "# apply hook to block\n",
    "hook_pos = utils.get_act_name(\"mlp_out\", layer)\n",
    "activation_dim = model.cfg.d_model # output dimension of the MLP\n",
    "dictionary_size = 16 * activation_dim * 4 if hidden_layer else 16 * activation_dim\n",
    "\n",
    "model_type = \"1_32768\" if hidden_layer else \"0_8192\"\n",
    "sae_dict = torch.load(f\"baulab.us/u/smarks/autoencoders/pythia-70m-deduped/mlp_out_layer{layer}/{model_type}/ae.pt\")\n",
    "\n",
    "sae = AutoEncoder(activation_dim, dictionary_size).to(device)\n",
    "sae.load_state_dict(state_dict=sae_dict)\n",
    "\n",
    "pre_sae_acts = []\n",
    "post_sae_acts = []\n",
    "\n",
    "# sae = AutoEncoder(activation_dim, dictionary_size*4).to(device)\n",
    "def apply_sae_hook(pattern, hook, sae, pre_sae_acts=None, post_sae_acts=None):\n",
    "    \"\"\"\n",
    "    During inference time, run SAE on the output of the specified layer, and feed it back in.\n",
    "    \"\"\"\n",
    "    if pre_sae_acts is not None:\n",
    "        pre_sae_acts.append(pattern.clone().cpu())\n",
    "    pattern = sae(pattern)\n",
    "    if post_sae_acts is not None:\n",
    "        post_sae_acts.append(pattern.clone().cpu())\n",
    "    return pattern\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5874611200000001 allocated, 81.45128652800001 reserved, 84.986691584 total\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "def display_memory():\n",
    "    total = torch.cuda.get_device_properties(0).total_memory\n",
    "    r = torch.cuda.memory_reserved(0)\n",
    "    a = torch.cuda.memory_allocated(0)\n",
    "    print(f\"{a*1e-9} allocated, {r*1e-9} reserved, {total*1e-9} total\")\n",
    "display_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fresh_sae = AutoEncoder(activation_dim, dictionary_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['hook_embed', 'blocks.0.hook_resid_pre', 'blocks.0.ln1.hook_scale', 'blocks.0.ln1.hook_normalized', 'blocks.0.attn.hook_q', 'blocks.0.attn.hook_k', 'blocks.0.attn.hook_v', 'blocks.0.attn.hook_rot_q', 'blocks.0.attn.hook_rot_k', 'blocks.0.attn.hook_attn_scores', 'blocks.0.attn.hook_pattern', 'blocks.0.attn.hook_z', 'blocks.0.hook_attn_out', 'blocks.0.hook_mlp_in', 'blocks.0.ln2.hook_scale', 'blocks.0.ln2.hook_normalized', 'blocks.0.mlp.hook_pre', 'blocks.0.mlp.hook_post', 'blocks.0.hook_mlp_out', 'blocks.0.hook_resid_post', 'blocks.1.hook_resid_pre', 'blocks.1.ln1.hook_scale', 'blocks.1.ln1.hook_normalized', 'blocks.1.attn.hook_q', 'blocks.1.attn.hook_k', 'blocks.1.attn.hook_v', 'blocks.1.attn.hook_rot_q', 'blocks.1.attn.hook_rot_k', 'blocks.1.attn.hook_attn_scores', 'blocks.1.attn.hook_pattern', 'blocks.1.attn.hook_z', 'blocks.1.hook_attn_out', 'blocks.1.hook_mlp_in', 'blocks.1.ln2.hook_scale', 'blocks.1.ln2.hook_normalized', 'blocks.1.mlp.hook_pre', 'blocks.1.mlp.hook_post', 'blocks.1.hook_mlp_out', 'blocks.1.hook_resid_post', 'blocks.2.hook_resid_pre', 'blocks.2.ln1.hook_scale', 'blocks.2.ln1.hook_normalized', 'blocks.2.attn.hook_q', 'blocks.2.attn.hook_k', 'blocks.2.attn.hook_v', 'blocks.2.attn.hook_rot_q', 'blocks.2.attn.hook_rot_k', 'blocks.2.attn.hook_attn_scores', 'blocks.2.attn.hook_pattern', 'blocks.2.attn.hook_z', 'blocks.2.hook_attn_out', 'blocks.2.hook_mlp_in', 'blocks.2.ln2.hook_scale', 'blocks.2.ln2.hook_normalized', 'blocks.2.mlp.hook_pre', 'blocks.2.mlp.hook_post', 'blocks.2.hook_mlp_out', 'blocks.2.hook_resid_post', 'blocks.3.hook_resid_pre', 'blocks.3.ln1.hook_scale', 'blocks.3.ln1.hook_normalized', 'blocks.3.attn.hook_q', 'blocks.3.attn.hook_k', 'blocks.3.attn.hook_v', 'blocks.3.attn.hook_rot_q', 'blocks.3.attn.hook_rot_k', 'blocks.3.attn.hook_attn_scores', 'blocks.3.attn.hook_pattern', 'blocks.3.attn.hook_z', 'blocks.3.hook_attn_out', 'blocks.3.hook_mlp_in', 'blocks.3.ln2.hook_scale', 'blocks.3.ln2.hook_normalized', 'blocks.3.mlp.hook_pre', 'blocks.3.mlp.hook_post', 'blocks.3.hook_mlp_out', 'blocks.3.hook_resid_post', 'blocks.4.hook_resid_pre', 'blocks.4.ln1.hook_scale', 'blocks.4.ln1.hook_normalized', 'blocks.4.attn.hook_q', 'blocks.4.attn.hook_k', 'blocks.4.attn.hook_v', 'blocks.4.attn.hook_rot_q', 'blocks.4.attn.hook_rot_k', 'blocks.4.attn.hook_attn_scores', 'blocks.4.attn.hook_pattern', 'blocks.4.attn.hook_z', 'blocks.4.hook_attn_out', 'blocks.4.hook_mlp_in', 'blocks.4.ln2.hook_scale', 'blocks.4.ln2.hook_normalized', 'blocks.4.mlp.hook_pre', 'blocks.4.mlp.hook_post', 'blocks.4.hook_mlp_out', 'blocks.4.hook_resid_post', 'blocks.5.hook_resid_pre', 'blocks.5.ln1.hook_scale', 'blocks.5.ln1.hook_normalized', 'blocks.5.attn.hook_q', 'blocks.5.attn.hook_k', 'blocks.5.attn.hook_v', 'blocks.5.attn.hook_rot_q', 'blocks.5.attn.hook_rot_k', 'blocks.5.attn.hook_attn_scores', 'blocks.5.attn.hook_pattern', 'blocks.5.attn.hook_z', 'blocks.5.hook_attn_out', 'blocks.5.hook_mlp_in', 'blocks.5.ln2.hook_scale', 'blocks.5.ln2.hook_normalized', 'blocks.5.mlp.hook_pre', 'blocks.5.mlp.hook_post', 'blocks.5.hook_mlp_out', 'blocks.5.hook_resid_post', 'ln_final.hook_scale', 'ln_final.hook_normalized'])\n",
      "torch.Size([1, 15, 512])\n"
     ]
    }
   ],
   "source": [
    "_, test_cache = model.run_with_cache(\n",
    "    tokenizer(next(ioi_task.train_iter)['text'], return_tensors='pt').input_ids[0],\n",
    "    )\n",
    "\n",
    "print(test_cache.keys())\n",
    "print(test_cache['blocks.1.hook_mlp_out'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>IOI Loss</th>\n",
       "      <th>IOI Accuracy</th>\n",
       "      <th>Sports Loss</th>\n",
       "      <th>Sports Accuracy</th>\n",
       "      <th>OWT Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>default model</td>\n",
       "      <td>5.787771</td>\n",
       "      <td>0.180</td>\n",
       "      <td>4.635437</td>\n",
       "      <td>0.347134</td>\n",
       "      <td>4.227315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pretrained sae</td>\n",
       "      <td>6.319211</td>\n",
       "      <td>0.220</td>\n",
       "      <td>4.951598</td>\n",
       "      <td>0.363057</td>\n",
       "      <td>4.373736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>random init 16x sae</td>\n",
       "      <td>6.259040</td>\n",
       "      <td>0.495</td>\n",
       "      <td>7.754953</td>\n",
       "      <td>0.340764</td>\n",
       "      <td>7.589589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  IOI Loss  IOI Accuracy  Sports Loss  Sports Accuracy  \\\n",
       "0        default model  5.787771         0.180     4.635437         0.347134   \n",
       "1       pretrained sae  6.319211         0.220     4.951598         0.363057   \n",
       "2  random init 16x sae  6.259040         0.495     7.754953         0.340764   \n",
       "\n",
       "   OWT Loss  \n",
       "0  4.227315  \n",
       "1  4.373736  \n",
       "2  7.589589  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# I'm pretty sure that these tasks also take an inference function, not just a model. That makes it more convenient to use run_with_hooks\n",
    "def sae_inference_fn(tokens, model=model, hook_name=hook_pos, sae=sae):\n",
    "    return model.run_with_hooks(\n",
    "        tokens,\n",
    "        fwd_hooks = [\n",
    "            (hook_name, lambda pattern, hook: apply_sae_hook(pattern, hook, sae,)) # pre_sae_acts, post_sae_acts))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "fresh_sae_inference_fn = lambda tokens: sae_inference_fn(tokens, model=model, hook_name=hook_pos, sae=fresh_sae)\n",
    "\n",
    "results = {\n",
    "    'Model': ['default model', 'pretrained sae', 'random init 16x sae'],\n",
    "    # 'Model': ['default model', 'random 64x sae', 'random 16x sae'],\n",
    "    'IOI Loss': [\n",
    "        ioi_task.get_test_loss(model).item(), \n",
    "        ioi_task.get_test_loss(sae_inference_fn).item(),\n",
    "        ioi_task.get_test_loss(fresh_sae_inference_fn).item()\n",
    "    ],\n",
    "    'IOI Accuracy': [\n",
    "        ioi_task.get_test_accuracy(model, check_all_logits=False), \n",
    "        ioi_task.get_test_accuracy(sae_inference_fn, check_all_logits=False),\n",
    "        ioi_task.get_test_accuracy(fresh_sae_inference_fn, check_all_logits=False)\n",
    "    ],\n",
    "    'Sports Loss': [\n",
    "        sports_task.get_test_loss(model).item(), \n",
    "        sports_task.get_test_loss(sae_inference_fn).item(),\n",
    "        sports_task.get_test_loss(fresh_sae_inference_fn).item()\n",
    "    ],\n",
    "    'Sports Accuracy': [\n",
    "        sports_task.get_test_accuracy(model, check_all_logits=False), \n",
    "        sports_task.get_test_accuracy(sae_inference_fn, check_all_logits=False),\n",
    "        sports_task.get_test_accuracy(fresh_sae_inference_fn, check_all_logits=False)\n",
    "    ],\n",
    "    'OWT Loss': [\n",
    "        owt_task.get_test_loss(model).item(), \n",
    "        owt_task.get_test_loss(sae_inference_fn).item(),\n",
    "        owt_task.get_test_loss(fresh_sae_inference_fn).item()\n",
    "    ],\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "display(results_df)\n",
    "\n",
    "# print(f\"IOI Loss: {ioi_task.get_test_loss(sae_inference_fn)}\")\n",
    "# print(f\"IOI Accuracy: {ioi_task.get_test_accuracy(sae_inference_fn, check_all_logits=False)}\")\n",
    "# print(f\"Sports Loss: {sports_task.get_test_loss(sae_inference_fn)}\")\n",
    "# print(f\"Sports Accuracy: {sports_task.get_test_accuracy(sae_inference_fn, check_all_logits=False)}\")\n",
    "# print(f\"OWT Loss: {owt_task.get_test_loss(sae_inference_fn)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prewritten SAE evaluation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dictionary_learning.evaluation import loss_recovered, evaluate\n",
    "from nnsight import LanguageModel\n",
    "from dictionary_learning.buffer import ActivationBuffer\n",
    "from dictionary_learning.training import trainSAE\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "nn_model = LanguageModel(\n",
    "    'EleutherAI/pythia-70m-deduped', # this can be any Huggingface model\n",
    "    device_map = 'cuda:0'\n",
    ")\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "# train_dataset = load_dataset('wikitext', 'wikitext-103-v1', split='train[:1000000]')\n",
    "train_dataset = load_dataset('Skylion007/openwebtext', split='train[:100]')\n",
    "def yield_sentences(data_split):\n",
    "    for example in data_split:\n",
    "        text = example['text']\n",
    "        sentences = text.split('\\n')\n",
    "        for sentence in sentences:\n",
    "            if sentence:  # skip empty lines\n",
    "                yield sentence\n",
    "\n",
    "# Creating an iterator for training sentences\n",
    "train_sentences = yield_sentences(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Port-au-Prince, Haiti (CNN) -- Earthquake victims, writhing in pain and grasping at life, watched doctors and nurses walk away from a field hospital Friday night after a Belgian medical team evacuated the area, saying it was concerned about security.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[next(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlrn",
   "language": "python",
   "name": "unlrn"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
