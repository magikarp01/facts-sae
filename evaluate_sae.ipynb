{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to evaluate SAEs when used in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use HookedTransformer hooks to replace the specified component output with it's SAE counterpart\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from transformer_lens import HookedTransformer, utils\n",
    "import torch\n",
    "import nnsight\n",
    "device = 'cuda'\n",
    "from tasks.ioi.IOITask import IOITask\n",
    "from tasks.facts.SportsTask import SportsTask\n",
    "from tasks.owt.OWTTask import OWTTask\n",
    "from tasks import PileTask\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-2.8b-deduped into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/phillip_guo/miniconda3/envs/unlrn-2/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for Skylion007/openwebtext contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/Skylion007/openwebtext\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    # 'EleutherAI/pythia-70m-deduped',\n",
    "    # 'EleutherAI/pythia-1.4b-deduped',\n",
    "    'EleutherAI/pythia-2.8b-deduped',\n",
    "    device=device\n",
    ")\n",
    "\n",
    "model.set_use_hook_mlp_in(True)\n",
    "tokenizer = model.tokenizer\n",
    "batch_size=500\n",
    "\n",
    "ioi_task = IOITask(batch_size=batch_size, tokenizer=tokenizer, device=device, handle_multitoken_labels=True, num_data=1000)\n",
    "sports_task = SportsTask(batch_size=batch_size, tokenizer=tokenizer, device=device)\n",
    "owt_task = OWTTask(batch_size=batch_size, tokenizer=tokenizer, device=device, ctx_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "from dictionary_learning.buffer import ActivationBuffer\n",
    "from dictionary_learning.training import trainSAE\n",
    "\n",
    "model = LanguageModel(\n",
    "    # 'EleutherAI/pythia-70m-deduped', # this can be any Huggingface model\n",
    "    'EleutherAI/pythia-2.8b-deduped',\n",
    "    device_map = 'cuda:0'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dictionary_learning.dictionary import AutoEncoder\n",
    "\n",
    "layer = 15\n",
    "hidden_layer = False # if True, use the hidden layer, else use the output layer\n",
    "# submodule = model.blocks[layer].mlp # layer 1 MLP\n",
    "submodule = model.gpt_neox.layers[layer].mlp # layer 1 MLP\n",
    "\n",
    "# apply hook to block\n",
    "hook_pos = utils.get_act_name(\"mlp_out\", layer)\n",
    "# activation_dim = model.cfg.d_model # output dimension of the MLP\n",
    "activation_dim = model.config.hidden_size # output dimension of the MLP\n",
    "dictionary_size = 16 * activation_dim * 4 if hidden_layer else 16 * activation_dim\n",
    "\n",
    "step = 490000\n",
    "# model_type = \"1_32768\" if hidden_layer else \"0_8192\"\n",
    "# sae_dict = torch.load(f\"baulab.us/u/smarks/autoencoders/pythia-70m-deduped/mlp_out_layer{layer}/{model_type}/ae.pt\")\n",
    "sae_dict = torch.load(f\"trained_saes/2.8b_l{layer}/checkpoints/ae_{step}.pt\")\n",
    "\n",
    "sae = AutoEncoder(activation_dim, dictionary_size).to(device)\n",
    "sae.load_state_dict(state_dict=sae_dict)\n",
    "\n",
    "pre_sae_acts = []\n",
    "post_sae_acts = []\n",
    "\n",
    "# sae = AutoEncoder(activation_dim, dictionary_size*4).to(device)\n",
    "def apply_sae_hook(pattern, hook, sae, pre_sae_acts=None, post_sae_acts=None):\n",
    "    \"\"\"\n",
    "    During inference time, run SAE on the output of the specified layer, and feed it back in.\n",
    "    \"\"\"\n",
    "    if pre_sae_acts is not None:\n",
    "        pre_sae_acts.append(pattern.clone().cpu())\n",
    "    pattern = sae(pattern)\n",
    "    if post_sae_acts is not None:\n",
    "        post_sae_acts.append(pattern.clone().cpu())\n",
    "    return pattern\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate with HookedTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.05828608 allocated, 13.071548416 reserved, 84.986691584 total\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "def display_memory():\n",
    "    total = torch.cuda.get_device_properties(0).total_memory\n",
    "    r = torch.cuda.memory_reserved(0)\n",
    "    a = torch.cuda.memory_allocated(0)\n",
    "    print(f\"{a*1e-9} allocated, {r*1e-9} reserved, {total*1e-9} total\")\n",
    "display_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fresh_sae = AutoEncoder(activation_dim, dictionary_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['hook_embed', 'blocks.0.hook_resid_pre', 'blocks.0.ln1.hook_scale', 'blocks.0.ln1.hook_normalized', 'blocks.0.attn.hook_q', 'blocks.0.attn.hook_k', 'blocks.0.attn.hook_v', 'blocks.0.attn.hook_rot_q', 'blocks.0.attn.hook_rot_k', 'blocks.0.attn.hook_attn_scores', 'blocks.0.attn.hook_pattern', 'blocks.0.attn.hook_z', 'blocks.0.hook_attn_out', 'blocks.0.hook_mlp_in', 'blocks.0.ln2.hook_scale', 'blocks.0.ln2.hook_normalized', 'blocks.0.mlp.hook_pre', 'blocks.0.mlp.hook_post', 'blocks.0.hook_mlp_out', 'blocks.0.hook_resid_post', 'blocks.1.hook_resid_pre', 'blocks.1.ln1.hook_scale', 'blocks.1.ln1.hook_normalized', 'blocks.1.attn.hook_q', 'blocks.1.attn.hook_k', 'blocks.1.attn.hook_v', 'blocks.1.attn.hook_rot_q', 'blocks.1.attn.hook_rot_k', 'blocks.1.attn.hook_attn_scores', 'blocks.1.attn.hook_pattern', 'blocks.1.attn.hook_z', 'blocks.1.hook_attn_out', 'blocks.1.hook_mlp_in', 'blocks.1.ln2.hook_scale', 'blocks.1.ln2.hook_normalized', 'blocks.1.mlp.hook_pre', 'blocks.1.mlp.hook_post', 'blocks.1.hook_mlp_out', 'blocks.1.hook_resid_post', 'blocks.2.hook_resid_pre', 'blocks.2.ln1.hook_scale', 'blocks.2.ln1.hook_normalized', 'blocks.2.attn.hook_q', 'blocks.2.attn.hook_k', 'blocks.2.attn.hook_v', 'blocks.2.attn.hook_rot_q', 'blocks.2.attn.hook_rot_k', 'blocks.2.attn.hook_attn_scores', 'blocks.2.attn.hook_pattern', 'blocks.2.attn.hook_z', 'blocks.2.hook_attn_out', 'blocks.2.hook_mlp_in', 'blocks.2.ln2.hook_scale', 'blocks.2.ln2.hook_normalized', 'blocks.2.mlp.hook_pre', 'blocks.2.mlp.hook_post', 'blocks.2.hook_mlp_out', 'blocks.2.hook_resid_post', 'blocks.3.hook_resid_pre', 'blocks.3.ln1.hook_scale', 'blocks.3.ln1.hook_normalized', 'blocks.3.attn.hook_q', 'blocks.3.attn.hook_k', 'blocks.3.attn.hook_v', 'blocks.3.attn.hook_rot_q', 'blocks.3.attn.hook_rot_k', 'blocks.3.attn.hook_attn_scores', 'blocks.3.attn.hook_pattern', 'blocks.3.attn.hook_z', 'blocks.3.hook_attn_out', 'blocks.3.hook_mlp_in', 'blocks.3.ln2.hook_scale', 'blocks.3.ln2.hook_normalized', 'blocks.3.mlp.hook_pre', 'blocks.3.mlp.hook_post', 'blocks.3.hook_mlp_out', 'blocks.3.hook_resid_post', 'blocks.4.hook_resid_pre', 'blocks.4.ln1.hook_scale', 'blocks.4.ln1.hook_normalized', 'blocks.4.attn.hook_q', 'blocks.4.attn.hook_k', 'blocks.4.attn.hook_v', 'blocks.4.attn.hook_rot_q', 'blocks.4.attn.hook_rot_k', 'blocks.4.attn.hook_attn_scores', 'blocks.4.attn.hook_pattern', 'blocks.4.attn.hook_z', 'blocks.4.hook_attn_out', 'blocks.4.hook_mlp_in', 'blocks.4.ln2.hook_scale', 'blocks.4.ln2.hook_normalized', 'blocks.4.mlp.hook_pre', 'blocks.4.mlp.hook_post', 'blocks.4.hook_mlp_out', 'blocks.4.hook_resid_post', 'blocks.5.hook_resid_pre', 'blocks.5.ln1.hook_scale', 'blocks.5.ln1.hook_normalized', 'blocks.5.attn.hook_q', 'blocks.5.attn.hook_k', 'blocks.5.attn.hook_v', 'blocks.5.attn.hook_rot_q', 'blocks.5.attn.hook_rot_k', 'blocks.5.attn.hook_attn_scores', 'blocks.5.attn.hook_pattern', 'blocks.5.attn.hook_z', 'blocks.5.hook_attn_out', 'blocks.5.hook_mlp_in', 'blocks.5.ln2.hook_scale', 'blocks.5.ln2.hook_normalized', 'blocks.5.mlp.hook_pre', 'blocks.5.mlp.hook_post', 'blocks.5.hook_mlp_out', 'blocks.5.hook_resid_post', 'blocks.6.hook_resid_pre', 'blocks.6.ln1.hook_scale', 'blocks.6.ln1.hook_normalized', 'blocks.6.attn.hook_q', 'blocks.6.attn.hook_k', 'blocks.6.attn.hook_v', 'blocks.6.attn.hook_rot_q', 'blocks.6.attn.hook_rot_k', 'blocks.6.attn.hook_attn_scores', 'blocks.6.attn.hook_pattern', 'blocks.6.attn.hook_z', 'blocks.6.hook_attn_out', 'blocks.6.hook_mlp_in', 'blocks.6.ln2.hook_scale', 'blocks.6.ln2.hook_normalized', 'blocks.6.mlp.hook_pre', 'blocks.6.mlp.hook_post', 'blocks.6.hook_mlp_out', 'blocks.6.hook_resid_post', 'blocks.7.hook_resid_pre', 'blocks.7.ln1.hook_scale', 'blocks.7.ln1.hook_normalized', 'blocks.7.attn.hook_q', 'blocks.7.attn.hook_k', 'blocks.7.attn.hook_v', 'blocks.7.attn.hook_rot_q', 'blocks.7.attn.hook_rot_k', 'blocks.7.attn.hook_attn_scores', 'blocks.7.attn.hook_pattern', 'blocks.7.attn.hook_z', 'blocks.7.hook_attn_out', 'blocks.7.hook_mlp_in', 'blocks.7.ln2.hook_scale', 'blocks.7.ln2.hook_normalized', 'blocks.7.mlp.hook_pre', 'blocks.7.mlp.hook_post', 'blocks.7.hook_mlp_out', 'blocks.7.hook_resid_post', 'blocks.8.hook_resid_pre', 'blocks.8.ln1.hook_scale', 'blocks.8.ln1.hook_normalized', 'blocks.8.attn.hook_q', 'blocks.8.attn.hook_k', 'blocks.8.attn.hook_v', 'blocks.8.attn.hook_rot_q', 'blocks.8.attn.hook_rot_k', 'blocks.8.attn.hook_attn_scores', 'blocks.8.attn.hook_pattern', 'blocks.8.attn.hook_z', 'blocks.8.hook_attn_out', 'blocks.8.hook_mlp_in', 'blocks.8.ln2.hook_scale', 'blocks.8.ln2.hook_normalized', 'blocks.8.mlp.hook_pre', 'blocks.8.mlp.hook_post', 'blocks.8.hook_mlp_out', 'blocks.8.hook_resid_post', 'blocks.9.hook_resid_pre', 'blocks.9.ln1.hook_scale', 'blocks.9.ln1.hook_normalized', 'blocks.9.attn.hook_q', 'blocks.9.attn.hook_k', 'blocks.9.attn.hook_v', 'blocks.9.attn.hook_rot_q', 'blocks.9.attn.hook_rot_k', 'blocks.9.attn.hook_attn_scores', 'blocks.9.attn.hook_pattern', 'blocks.9.attn.hook_z', 'blocks.9.hook_attn_out', 'blocks.9.hook_mlp_in', 'blocks.9.ln2.hook_scale', 'blocks.9.ln2.hook_normalized', 'blocks.9.mlp.hook_pre', 'blocks.9.mlp.hook_post', 'blocks.9.hook_mlp_out', 'blocks.9.hook_resid_post', 'blocks.10.hook_resid_pre', 'blocks.10.ln1.hook_scale', 'blocks.10.ln1.hook_normalized', 'blocks.10.attn.hook_q', 'blocks.10.attn.hook_k', 'blocks.10.attn.hook_v', 'blocks.10.attn.hook_rot_q', 'blocks.10.attn.hook_rot_k', 'blocks.10.attn.hook_attn_scores', 'blocks.10.attn.hook_pattern', 'blocks.10.attn.hook_z', 'blocks.10.hook_attn_out', 'blocks.10.hook_mlp_in', 'blocks.10.ln2.hook_scale', 'blocks.10.ln2.hook_normalized', 'blocks.10.mlp.hook_pre', 'blocks.10.mlp.hook_post', 'blocks.10.hook_mlp_out', 'blocks.10.hook_resid_post', 'blocks.11.hook_resid_pre', 'blocks.11.ln1.hook_scale', 'blocks.11.ln1.hook_normalized', 'blocks.11.attn.hook_q', 'blocks.11.attn.hook_k', 'blocks.11.attn.hook_v', 'blocks.11.attn.hook_rot_q', 'blocks.11.attn.hook_rot_k', 'blocks.11.attn.hook_attn_scores', 'blocks.11.attn.hook_pattern', 'blocks.11.attn.hook_z', 'blocks.11.hook_attn_out', 'blocks.11.hook_mlp_in', 'blocks.11.ln2.hook_scale', 'blocks.11.ln2.hook_normalized', 'blocks.11.mlp.hook_pre', 'blocks.11.mlp.hook_post', 'blocks.11.hook_mlp_out', 'blocks.11.hook_resid_post', 'blocks.12.hook_resid_pre', 'blocks.12.ln1.hook_scale', 'blocks.12.ln1.hook_normalized', 'blocks.12.attn.hook_q', 'blocks.12.attn.hook_k', 'blocks.12.attn.hook_v', 'blocks.12.attn.hook_rot_q', 'blocks.12.attn.hook_rot_k', 'blocks.12.attn.hook_attn_scores', 'blocks.12.attn.hook_pattern', 'blocks.12.attn.hook_z', 'blocks.12.hook_attn_out', 'blocks.12.hook_mlp_in', 'blocks.12.ln2.hook_scale', 'blocks.12.ln2.hook_normalized', 'blocks.12.mlp.hook_pre', 'blocks.12.mlp.hook_post', 'blocks.12.hook_mlp_out', 'blocks.12.hook_resid_post', 'blocks.13.hook_resid_pre', 'blocks.13.ln1.hook_scale', 'blocks.13.ln1.hook_normalized', 'blocks.13.attn.hook_q', 'blocks.13.attn.hook_k', 'blocks.13.attn.hook_v', 'blocks.13.attn.hook_rot_q', 'blocks.13.attn.hook_rot_k', 'blocks.13.attn.hook_attn_scores', 'blocks.13.attn.hook_pattern', 'blocks.13.attn.hook_z', 'blocks.13.hook_attn_out', 'blocks.13.hook_mlp_in', 'blocks.13.ln2.hook_scale', 'blocks.13.ln2.hook_normalized', 'blocks.13.mlp.hook_pre', 'blocks.13.mlp.hook_post', 'blocks.13.hook_mlp_out', 'blocks.13.hook_resid_post', 'blocks.14.hook_resid_pre', 'blocks.14.ln1.hook_scale', 'blocks.14.ln1.hook_normalized', 'blocks.14.attn.hook_q', 'blocks.14.attn.hook_k', 'blocks.14.attn.hook_v', 'blocks.14.attn.hook_rot_q', 'blocks.14.attn.hook_rot_k', 'blocks.14.attn.hook_attn_scores', 'blocks.14.attn.hook_pattern', 'blocks.14.attn.hook_z', 'blocks.14.hook_attn_out', 'blocks.14.hook_mlp_in', 'blocks.14.ln2.hook_scale', 'blocks.14.ln2.hook_normalized', 'blocks.14.mlp.hook_pre', 'blocks.14.mlp.hook_post', 'blocks.14.hook_mlp_out', 'blocks.14.hook_resid_post', 'blocks.15.hook_resid_pre', 'blocks.15.ln1.hook_scale', 'blocks.15.ln1.hook_normalized', 'blocks.15.attn.hook_q', 'blocks.15.attn.hook_k', 'blocks.15.attn.hook_v', 'blocks.15.attn.hook_rot_q', 'blocks.15.attn.hook_rot_k', 'blocks.15.attn.hook_attn_scores', 'blocks.15.attn.hook_pattern', 'blocks.15.attn.hook_z', 'blocks.15.hook_attn_out', 'blocks.15.hook_mlp_in', 'blocks.15.ln2.hook_scale', 'blocks.15.ln2.hook_normalized', 'blocks.15.mlp.hook_pre', 'blocks.15.mlp.hook_post', 'blocks.15.hook_mlp_out', 'blocks.15.hook_resid_post', 'blocks.16.hook_resid_pre', 'blocks.16.ln1.hook_scale', 'blocks.16.ln1.hook_normalized', 'blocks.16.attn.hook_q', 'blocks.16.attn.hook_k', 'blocks.16.attn.hook_v', 'blocks.16.attn.hook_rot_q', 'blocks.16.attn.hook_rot_k', 'blocks.16.attn.hook_attn_scores', 'blocks.16.attn.hook_pattern', 'blocks.16.attn.hook_z', 'blocks.16.hook_attn_out', 'blocks.16.hook_mlp_in', 'blocks.16.ln2.hook_scale', 'blocks.16.ln2.hook_normalized', 'blocks.16.mlp.hook_pre', 'blocks.16.mlp.hook_post', 'blocks.16.hook_mlp_out', 'blocks.16.hook_resid_post', 'blocks.17.hook_resid_pre', 'blocks.17.ln1.hook_scale', 'blocks.17.ln1.hook_normalized', 'blocks.17.attn.hook_q', 'blocks.17.attn.hook_k', 'blocks.17.attn.hook_v', 'blocks.17.attn.hook_rot_q', 'blocks.17.attn.hook_rot_k', 'blocks.17.attn.hook_attn_scores', 'blocks.17.attn.hook_pattern', 'blocks.17.attn.hook_z', 'blocks.17.hook_attn_out', 'blocks.17.hook_mlp_in', 'blocks.17.ln2.hook_scale', 'blocks.17.ln2.hook_normalized', 'blocks.17.mlp.hook_pre', 'blocks.17.mlp.hook_post', 'blocks.17.hook_mlp_out', 'blocks.17.hook_resid_post', 'blocks.18.hook_resid_pre', 'blocks.18.ln1.hook_scale', 'blocks.18.ln1.hook_normalized', 'blocks.18.attn.hook_q', 'blocks.18.attn.hook_k', 'blocks.18.attn.hook_v', 'blocks.18.attn.hook_rot_q', 'blocks.18.attn.hook_rot_k', 'blocks.18.attn.hook_attn_scores', 'blocks.18.attn.hook_pattern', 'blocks.18.attn.hook_z', 'blocks.18.hook_attn_out', 'blocks.18.hook_mlp_in', 'blocks.18.ln2.hook_scale', 'blocks.18.ln2.hook_normalized', 'blocks.18.mlp.hook_pre', 'blocks.18.mlp.hook_post', 'blocks.18.hook_mlp_out', 'blocks.18.hook_resid_post', 'blocks.19.hook_resid_pre', 'blocks.19.ln1.hook_scale', 'blocks.19.ln1.hook_normalized', 'blocks.19.attn.hook_q', 'blocks.19.attn.hook_k', 'blocks.19.attn.hook_v', 'blocks.19.attn.hook_rot_q', 'blocks.19.attn.hook_rot_k', 'blocks.19.attn.hook_attn_scores', 'blocks.19.attn.hook_pattern', 'blocks.19.attn.hook_z', 'blocks.19.hook_attn_out', 'blocks.19.hook_mlp_in', 'blocks.19.ln2.hook_scale', 'blocks.19.ln2.hook_normalized', 'blocks.19.mlp.hook_pre', 'blocks.19.mlp.hook_post', 'blocks.19.hook_mlp_out', 'blocks.19.hook_resid_post', 'blocks.20.hook_resid_pre', 'blocks.20.ln1.hook_scale', 'blocks.20.ln1.hook_normalized', 'blocks.20.attn.hook_q', 'blocks.20.attn.hook_k', 'blocks.20.attn.hook_v', 'blocks.20.attn.hook_rot_q', 'blocks.20.attn.hook_rot_k', 'blocks.20.attn.hook_attn_scores', 'blocks.20.attn.hook_pattern', 'blocks.20.attn.hook_z', 'blocks.20.hook_attn_out', 'blocks.20.hook_mlp_in', 'blocks.20.ln2.hook_scale', 'blocks.20.ln2.hook_normalized', 'blocks.20.mlp.hook_pre', 'blocks.20.mlp.hook_post', 'blocks.20.hook_mlp_out', 'blocks.20.hook_resid_post', 'blocks.21.hook_resid_pre', 'blocks.21.ln1.hook_scale', 'blocks.21.ln1.hook_normalized', 'blocks.21.attn.hook_q', 'blocks.21.attn.hook_k', 'blocks.21.attn.hook_v', 'blocks.21.attn.hook_rot_q', 'blocks.21.attn.hook_rot_k', 'blocks.21.attn.hook_attn_scores', 'blocks.21.attn.hook_pattern', 'blocks.21.attn.hook_z', 'blocks.21.hook_attn_out', 'blocks.21.hook_mlp_in', 'blocks.21.ln2.hook_scale', 'blocks.21.ln2.hook_normalized', 'blocks.21.mlp.hook_pre', 'blocks.21.mlp.hook_post', 'blocks.21.hook_mlp_out', 'blocks.21.hook_resid_post', 'blocks.22.hook_resid_pre', 'blocks.22.ln1.hook_scale', 'blocks.22.ln1.hook_normalized', 'blocks.22.attn.hook_q', 'blocks.22.attn.hook_k', 'blocks.22.attn.hook_v', 'blocks.22.attn.hook_rot_q', 'blocks.22.attn.hook_rot_k', 'blocks.22.attn.hook_attn_scores', 'blocks.22.attn.hook_pattern', 'blocks.22.attn.hook_z', 'blocks.22.hook_attn_out', 'blocks.22.hook_mlp_in', 'blocks.22.ln2.hook_scale', 'blocks.22.ln2.hook_normalized', 'blocks.22.mlp.hook_pre', 'blocks.22.mlp.hook_post', 'blocks.22.hook_mlp_out', 'blocks.22.hook_resid_post', 'blocks.23.hook_resid_pre', 'blocks.23.ln1.hook_scale', 'blocks.23.ln1.hook_normalized', 'blocks.23.attn.hook_q', 'blocks.23.attn.hook_k', 'blocks.23.attn.hook_v', 'blocks.23.attn.hook_rot_q', 'blocks.23.attn.hook_rot_k', 'blocks.23.attn.hook_attn_scores', 'blocks.23.attn.hook_pattern', 'blocks.23.attn.hook_z', 'blocks.23.hook_attn_out', 'blocks.23.hook_mlp_in', 'blocks.23.ln2.hook_scale', 'blocks.23.ln2.hook_normalized', 'blocks.23.mlp.hook_pre', 'blocks.23.mlp.hook_post', 'blocks.23.hook_mlp_out', 'blocks.23.hook_resid_post', 'blocks.24.hook_resid_pre', 'blocks.24.ln1.hook_scale', 'blocks.24.ln1.hook_normalized', 'blocks.24.attn.hook_q', 'blocks.24.attn.hook_k', 'blocks.24.attn.hook_v', 'blocks.24.attn.hook_rot_q', 'blocks.24.attn.hook_rot_k', 'blocks.24.attn.hook_attn_scores', 'blocks.24.attn.hook_pattern', 'blocks.24.attn.hook_z', 'blocks.24.hook_attn_out', 'blocks.24.hook_mlp_in', 'blocks.24.ln2.hook_scale', 'blocks.24.ln2.hook_normalized', 'blocks.24.mlp.hook_pre', 'blocks.24.mlp.hook_post', 'blocks.24.hook_mlp_out', 'blocks.24.hook_resid_post', 'blocks.25.hook_resid_pre', 'blocks.25.ln1.hook_scale', 'blocks.25.ln1.hook_normalized', 'blocks.25.attn.hook_q', 'blocks.25.attn.hook_k', 'blocks.25.attn.hook_v', 'blocks.25.attn.hook_rot_q', 'blocks.25.attn.hook_rot_k', 'blocks.25.attn.hook_attn_scores', 'blocks.25.attn.hook_pattern', 'blocks.25.attn.hook_z', 'blocks.25.hook_attn_out', 'blocks.25.hook_mlp_in', 'blocks.25.ln2.hook_scale', 'blocks.25.ln2.hook_normalized', 'blocks.25.mlp.hook_pre', 'blocks.25.mlp.hook_post', 'blocks.25.hook_mlp_out', 'blocks.25.hook_resid_post', 'blocks.26.hook_resid_pre', 'blocks.26.ln1.hook_scale', 'blocks.26.ln1.hook_normalized', 'blocks.26.attn.hook_q', 'blocks.26.attn.hook_k', 'blocks.26.attn.hook_v', 'blocks.26.attn.hook_rot_q', 'blocks.26.attn.hook_rot_k', 'blocks.26.attn.hook_attn_scores', 'blocks.26.attn.hook_pattern', 'blocks.26.attn.hook_z', 'blocks.26.hook_attn_out', 'blocks.26.hook_mlp_in', 'blocks.26.ln2.hook_scale', 'blocks.26.ln2.hook_normalized', 'blocks.26.mlp.hook_pre', 'blocks.26.mlp.hook_post', 'blocks.26.hook_mlp_out', 'blocks.26.hook_resid_post', 'blocks.27.hook_resid_pre', 'blocks.27.ln1.hook_scale', 'blocks.27.ln1.hook_normalized', 'blocks.27.attn.hook_q', 'blocks.27.attn.hook_k', 'blocks.27.attn.hook_v', 'blocks.27.attn.hook_rot_q', 'blocks.27.attn.hook_rot_k', 'blocks.27.attn.hook_attn_scores', 'blocks.27.attn.hook_pattern', 'blocks.27.attn.hook_z', 'blocks.27.hook_attn_out', 'blocks.27.hook_mlp_in', 'blocks.27.ln2.hook_scale', 'blocks.27.ln2.hook_normalized', 'blocks.27.mlp.hook_pre', 'blocks.27.mlp.hook_post', 'blocks.27.hook_mlp_out', 'blocks.27.hook_resid_post', 'blocks.28.hook_resid_pre', 'blocks.28.ln1.hook_scale', 'blocks.28.ln1.hook_normalized', 'blocks.28.attn.hook_q', 'blocks.28.attn.hook_k', 'blocks.28.attn.hook_v', 'blocks.28.attn.hook_rot_q', 'blocks.28.attn.hook_rot_k', 'blocks.28.attn.hook_attn_scores', 'blocks.28.attn.hook_pattern', 'blocks.28.attn.hook_z', 'blocks.28.hook_attn_out', 'blocks.28.hook_mlp_in', 'blocks.28.ln2.hook_scale', 'blocks.28.ln2.hook_normalized', 'blocks.28.mlp.hook_pre', 'blocks.28.mlp.hook_post', 'blocks.28.hook_mlp_out', 'blocks.28.hook_resid_post', 'blocks.29.hook_resid_pre', 'blocks.29.ln1.hook_scale', 'blocks.29.ln1.hook_normalized', 'blocks.29.attn.hook_q', 'blocks.29.attn.hook_k', 'blocks.29.attn.hook_v', 'blocks.29.attn.hook_rot_q', 'blocks.29.attn.hook_rot_k', 'blocks.29.attn.hook_attn_scores', 'blocks.29.attn.hook_pattern', 'blocks.29.attn.hook_z', 'blocks.29.hook_attn_out', 'blocks.29.hook_mlp_in', 'blocks.29.ln2.hook_scale', 'blocks.29.ln2.hook_normalized', 'blocks.29.mlp.hook_pre', 'blocks.29.mlp.hook_post', 'blocks.29.hook_mlp_out', 'blocks.29.hook_resid_post', 'blocks.30.hook_resid_pre', 'blocks.30.ln1.hook_scale', 'blocks.30.ln1.hook_normalized', 'blocks.30.attn.hook_q', 'blocks.30.attn.hook_k', 'blocks.30.attn.hook_v', 'blocks.30.attn.hook_rot_q', 'blocks.30.attn.hook_rot_k', 'blocks.30.attn.hook_attn_scores', 'blocks.30.attn.hook_pattern', 'blocks.30.attn.hook_z', 'blocks.30.hook_attn_out', 'blocks.30.hook_mlp_in', 'blocks.30.ln2.hook_scale', 'blocks.30.ln2.hook_normalized', 'blocks.30.mlp.hook_pre', 'blocks.30.mlp.hook_post', 'blocks.30.hook_mlp_out', 'blocks.30.hook_resid_post', 'blocks.31.hook_resid_pre', 'blocks.31.ln1.hook_scale', 'blocks.31.ln1.hook_normalized', 'blocks.31.attn.hook_q', 'blocks.31.attn.hook_k', 'blocks.31.attn.hook_v', 'blocks.31.attn.hook_rot_q', 'blocks.31.attn.hook_rot_k', 'blocks.31.attn.hook_attn_scores', 'blocks.31.attn.hook_pattern', 'blocks.31.attn.hook_z', 'blocks.31.hook_attn_out', 'blocks.31.hook_mlp_in', 'blocks.31.ln2.hook_scale', 'blocks.31.ln2.hook_normalized', 'blocks.31.mlp.hook_pre', 'blocks.31.mlp.hook_post', 'blocks.31.hook_mlp_out', 'blocks.31.hook_resid_post', 'ln_final.hook_scale', 'ln_final.hook_normalized'])\n",
      "torch.Size([1, 15, 2560])\n"
     ]
    }
   ],
   "source": [
    "_, test_cache = model.run_with_cache(\n",
    "    tokenizer(next(ioi_task.train_iter)['text'], return_tensors='pt').input_ids[0],\n",
    "    )\n",
    "\n",
    "print(test_cache.keys())\n",
    "print(test_cache['blocks.1.hook_mlp_out'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>IOI Loss</th>\n",
       "      <th>IOI Accuracy</th>\n",
       "      <th>Sports Loss</th>\n",
       "      <th>Sports Accuracy</th>\n",
       "      <th>OWT Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>default model</td>\n",
       "      <td>1.595651</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.119766</td>\n",
       "      <td>0.996815</td>\n",
       "      <td>2.850236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pretrained sae</td>\n",
       "      <td>5.151785</td>\n",
       "      <td>0.670</td>\n",
       "      <td>4.219798</td>\n",
       "      <td>0.487261</td>\n",
       "      <td>5.483761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>random init 16x sae</td>\n",
       "      <td>2.656597</td>\n",
       "      <td>0.965</td>\n",
       "      <td>3.830590</td>\n",
       "      <td>0.404459</td>\n",
       "      <td>3.871112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  IOI Loss  IOI Accuracy  Sports Loss  Sports Accuracy  \\\n",
       "0        default model  1.595651         1.000     0.119766         0.996815   \n",
       "1       pretrained sae  5.151785         0.670     4.219798         0.487261   \n",
       "2  random init 16x sae  2.656597         0.965     3.830590         0.404459   \n",
       "\n",
       "   OWT Loss  \n",
       "0  2.850236  \n",
       "1  5.483761  \n",
       "2  3.871112  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# I'm pretty sure that these tasks also take an inference function, not just a model. That makes it more convenient to use run_with_hooks\n",
    "def sae_inference_fn(tokens, model=model, hook_name=hook_pos, sae=sae):\n",
    "    return model.run_with_hooks(\n",
    "        tokens,\n",
    "        fwd_hooks = [\n",
    "            (hook_name, lambda pattern, hook: apply_sae_hook(pattern, hook, sae,)) # pre_sae_acts, post_sae_acts))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "fresh_sae_inference_fn = lambda tokens: sae_inference_fn(tokens, model=model, hook_name=hook_pos, sae=fresh_sae)\n",
    "\n",
    "results = {\n",
    "    'Model': ['default model', 'pretrained sae', 'random init 16x sae'],\n",
    "    # 'Model': ['default model', 'random 64x sae', 'random 16x sae'],\n",
    "    'IOI Loss': [\n",
    "        ioi_task.get_test_loss(model).item(), \n",
    "        ioi_task.get_test_loss(sae_inference_fn).item(),\n",
    "        ioi_task.get_test_loss(fresh_sae_inference_fn).item()\n",
    "    ],\n",
    "    'IOI Accuracy': [\n",
    "        ioi_task.get_test_accuracy(model, check_all_logits=False), \n",
    "        ioi_task.get_test_accuracy(sae_inference_fn, check_all_logits=False),\n",
    "        ioi_task.get_test_accuracy(fresh_sae_inference_fn, check_all_logits=False)\n",
    "    ],\n",
    "    'Sports Loss': [\n",
    "        sports_task.get_test_loss(model).item(), \n",
    "        sports_task.get_test_loss(sae_inference_fn).item(),\n",
    "        sports_task.get_test_loss(fresh_sae_inference_fn).item()\n",
    "    ],\n",
    "    'Sports Accuracy': [\n",
    "        sports_task.get_test_accuracy(model, check_all_logits=False), \n",
    "        sports_task.get_test_accuracy(sae_inference_fn, check_all_logits=False),\n",
    "        sports_task.get_test_accuracy(fresh_sae_inference_fn, check_all_logits=False)\n",
    "    ],\n",
    "    'OWT Loss': [\n",
    "        owt_task.get_test_loss(model).item(), \n",
    "        owt_task.get_test_loss(sae_inference_fn).item(),\n",
    "        owt_task.get_test_loss(fresh_sae_inference_fn).item()\n",
    "    ],\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "display(results_df)\n",
    "\n",
    "# print(f\"IOI Loss: {ioi_task.get_test_loss(sae_inference_fn)}\")\n",
    "# print(f\"IOI Accuracy: {ioi_task.get_test_accuracy(sae_inference_fn, check_all_logits=False)}\")\n",
    "# print(f\"Sports Loss: {sports_task.get_test_loss(sae_inference_fn)}\")\n",
    "# print(f\"Sports Accuracy: {sports_task.get_test_accuracy(sae_inference_fn, check_all_logits=False)}\")\n",
    "# print(f\"OWT Loss: {owt_task.get_test_loss(sae_inference_fn)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prewritten SAE evaluation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb8de453d99648698795ac1cc30148b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "train_dataset = load_dataset('monology/pile-uncopyrighted', split='train', streaming=True)\n",
    "\n",
    "def yield_sentences(data_split, cycle=False):\n",
    "    while True:\n",
    "        for example in data_split:\n",
    "            text = example['text']\n",
    "            # sentences = text.split('\\n')\n",
    "            # for sentence in sentences:\n",
    "            #     if sentence:  # skip empty lines\n",
    "            #         yield sentence\n",
    "            yield text\n",
    "        if not cycle:\n",
    "            break\n",
    "\n",
    "# Creating an iterator for training sentences\n",
    "train_sentences = yield_sentences(train_dataset, cycle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=64\n",
    "from dictionary_learning.buffer import ActivationBuffer\n",
    "buffer = ActivationBuffer(\n",
    "    train_sentences,\n",
    "    model,\n",
    "    submodule,\n",
    "    out_feats=activation_dim, # output dimension of the model component\n",
    "    n_ctxs=1e3,\n",
    "    in_batch_size=int(BATCH_SIZE*2), # batch size for the model\n",
    "    out_batch_size=BATCH_SIZE*4, # batch size for the buffer\n",
    ") # buffer will return batches of tensors of dimension = submodule's output dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "refreshing buffer...\n",
      "buffer size: 0, need 128000.0, buffer_shape: torch.Size([0, 2560])\n",
      "max vram usages: [67.0]\n",
      "buffer size: 15586, need 128000.0, buffer_shape: torch.Size([15586, 2560])\n",
      "max vram usages: [67.0]\n",
      "buffer size: 31626, need 128000.0, buffer_shape: torch.Size([31626, 2560])\n",
      "max vram usages: [67.0]\n",
      "buffer size: 47410, need 128000.0, buffer_shape: torch.Size([47410, 2560])\n",
      "max vram usages: [67.0]\n",
      "buffer size: 62996, need 128000.0, buffer_shape: torch.Size([62996, 2560])\n",
      "max vram usages: [67.0]\n",
      "buffer size: 78599, need 128000.0, buffer_shape: torch.Size([78599, 2560])\n",
      "max vram usages: [67.0]\n",
      "buffer size: 94252, need 128000.0, buffer_shape: torch.Size([94252, 2560])\n",
      "max vram usages: [67.0]\n",
      "buffer size: 109790, need 128000.0, buffer_shape: torch.Size([109790, 2560])\n",
      "max vram usages: [67.0]\n",
      "buffer size: 125257, need 128000.0, buffer_shape: torch.Size([125257, 2560])\n",
      "max vram usages: [67.0]\n",
      "buffer refreshed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:46<00:00, 22.63s/it]\n"
     ]
    }
   ],
   "source": [
    "from dictionary_learning.evaluation import loss_recovered, evaluate\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "results_dict = defaultdict(list)\n",
    "n_iters = 10\n",
    "\n",
    "for i in tqdm(range(n_iters)):\n",
    "    out = evaluate(model, submodule, sae, buffer, device='cuda')\n",
    "    for k, v in out.items():\n",
    "        results_dict[k].append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mse_loss</th>\n",
       "      <th>sparsity_loss</th>\n",
       "      <th>l0</th>\n",
       "      <th>percent_alive</th>\n",
       "      <th>loss_original</th>\n",
       "      <th>loss_reconstructed</th>\n",
       "      <th>loss_zero</th>\n",
       "      <th>percent_recovered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.074379</td>\n",
       "      <td>13.287188</td>\n",
       "      <td>12.753906</td>\n",
       "      <td>0.037061</td>\n",
       "      <td>2.199511</td>\n",
       "      <td>2.222114</td>\n",
       "      <td>2.241945</td>\n",
       "      <td>0.467325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.071990</td>\n",
       "      <td>13.220024</td>\n",
       "      <td>11.953125</td>\n",
       "      <td>0.036377</td>\n",
       "      <td>2.271739</td>\n",
       "      <td>2.296409</td>\n",
       "      <td>2.321465</td>\n",
       "      <td>0.503888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.072716</td>\n",
       "      <td>13.056200</td>\n",
       "      <td>12.382812</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>2.331593</td>\n",
       "      <td>2.354844</td>\n",
       "      <td>2.374335</td>\n",
       "      <td>0.456003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.075491</td>\n",
       "      <td>13.465338</td>\n",
       "      <td>13.179688</td>\n",
       "      <td>0.037988</td>\n",
       "      <td>2.083156</td>\n",
       "      <td>2.110054</td>\n",
       "      <td>2.135535</td>\n",
       "      <td>0.486465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.073226</td>\n",
       "      <td>13.112363</td>\n",
       "      <td>12.445312</td>\n",
       "      <td>0.036768</td>\n",
       "      <td>2.311514</td>\n",
       "      <td>2.330863</td>\n",
       "      <td>2.350458</td>\n",
       "      <td>0.503153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.071450</td>\n",
       "      <td>13.129960</td>\n",
       "      <td>11.722656</td>\n",
       "      <td>0.035767</td>\n",
       "      <td>2.236460</td>\n",
       "      <td>2.259686</td>\n",
       "      <td>2.279502</td>\n",
       "      <td>0.460383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.077269</td>\n",
       "      <td>13.368313</td>\n",
       "      <td>13.277344</td>\n",
       "      <td>0.037256</td>\n",
       "      <td>2.230863</td>\n",
       "      <td>2.250642</td>\n",
       "      <td>2.272073</td>\n",
       "      <td>0.520044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.072811</td>\n",
       "      <td>13.384796</td>\n",
       "      <td>12.277344</td>\n",
       "      <td>0.037817</td>\n",
       "      <td>2.252879</td>\n",
       "      <td>2.273166</td>\n",
       "      <td>2.292691</td>\n",
       "      <td>0.490427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.075237</td>\n",
       "      <td>13.013618</td>\n",
       "      <td>12.820312</td>\n",
       "      <td>0.037671</td>\n",
       "      <td>2.290040</td>\n",
       "      <td>2.310185</td>\n",
       "      <td>2.329671</td>\n",
       "      <td>0.491701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.073348</td>\n",
       "      <td>13.502811</td>\n",
       "      <td>13.277344</td>\n",
       "      <td>0.038965</td>\n",
       "      <td>2.256372</td>\n",
       "      <td>2.280337</td>\n",
       "      <td>2.301687</td>\n",
       "      <td>0.471160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mse_loss  sparsity_loss         l0  percent_alive  loss_original  \\\n",
       "0  0.074379      13.287188  12.753906       0.037061       2.199511   \n",
       "1  0.071990      13.220024  11.953125       0.036377       2.271739   \n",
       "2  0.072716      13.056200  12.382812       0.037500       2.331593   \n",
       "3  0.075491      13.465338  13.179688       0.037988       2.083156   \n",
       "4  0.073226      13.112363  12.445312       0.036768       2.311514   \n",
       "5  0.071450      13.129960  11.722656       0.035767       2.236460   \n",
       "6  0.077269      13.368313  13.277344       0.037256       2.230863   \n",
       "7  0.072811      13.384796  12.277344       0.037817       2.252879   \n",
       "8  0.075237      13.013618  12.820312       0.037671       2.290040   \n",
       "9  0.073348      13.502811  13.277344       0.038965       2.256372   \n",
       "\n",
       "   loss_reconstructed  loss_zero  percent_recovered  \n",
       "0            2.222114   2.241945           0.467325  \n",
       "1            2.296409   2.321465           0.503888  \n",
       "2            2.354844   2.374335           0.456003  \n",
       "3            2.110054   2.135535           0.486465  \n",
       "4            2.330863   2.350458           0.503153  \n",
       "5            2.259686   2.279502           0.460383  \n",
       "6            2.250642   2.272073           0.520044  \n",
       "7            2.273166   2.292691           0.490427  \n",
       "8            2.310185   2.329671           0.491701  \n",
       "9            2.280337   2.301687           0.471160  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# turn the results into a dataframe\n",
    "results_df = pd.DataFrame(results_dict)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Layer 15, Step 490000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "mse_loss               0.073792\n",
       "sparsity_loss         13.254061\n",
       "l0                    12.608984\n",
       "percent_alive          0.037317\n",
       "loss_original          2.246413\n",
       "loss_reconstructed     2.268830\n",
       "loss_zero              2.289936\n",
       "percent_recovered      0.485055\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get mean of each column\n",
    "print(f\"On Layer {layer}, Step {step}\")\n",
    "mean_results = results_df.mean()\n",
    "display(mean_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4208215216.py, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[16], line 17\u001b[0;36m\u001b[0m\n\u001b[0;31m    train_dataset =\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from dictionary_learning.evaluation import loss_recovered, evaluate\n",
    "from nnsight import LanguageModel\n",
    "from dictionary_learning.buffer import ActivationBuffer\n",
    "from dictionary_learning.training import trainSAE\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# nn_model = LanguageModel(\n",
    "#     'EleutherAI/pythia-70m-deduped', # this can be any Huggingface model\n",
    "#     device_map = 'cuda:0'\n",
    "# )\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "# train_dataset = load_dataset('wikitext', 'wikitext-103-v1', split='train[:1000000]')\n",
    "# train_dataset = load_dataset('Skylion007/openwebtext', split='train[:100]')\n",
    "train_dataset = \n",
    "def yield_sentences(data_split):\n",
    "    for example in data_split:\n",
    "        text = example['text']\n",
    "        sentences = text.split('\\n')\n",
    "        for sentence in sentences:\n",
    "            if sentence:  # skip empty lines\n",
    "                yield sentence\n",
    "\n",
    "# Creating an iterator for training sentences\n",
    "train_sentences = yield_sentences(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tasks.owt.OWTTask import OWTTask\n",
    "owt = OWTTask(batch_size=10, tokenizer=tokenizer, device=device, ctx_length=50)\n",
    "test_batch = next(owt.test_iter)\n",
    "\n",
    "\n",
    "from dictionary_learning.dictionary import AutoEncoder\n",
    "\n",
    "layer = 1\n",
    "hidden_layer = False # if True, use the hidden layer, else use the output layer\n",
    "submodule = nn_model.gpt_neox.layers[1].mlp # layer 1 MLP\n",
    "# apply hook to block\n",
    "hook_pos = utils.get_act_name(\"mlp_out\", layer)\n",
    "activation_dim = model.cfg.d_model # output dimension of the MLP\n",
    "dictionary_size = 16 * activation_dim * 4 if hidden_layer else 16 * activation_dim\n",
    "\n",
    "model_type = \"1_32768\" if hidden_layer else \"0_8192\"\n",
    "sae_dict = torch.load(f\"baulab.us/u/smarks/autoencoders/pythia-70m-deduped/mlp_out_layer{layer}/{model_type}/ae.pt\")\n",
    "\n",
    "sae = AutoEncoder(activation_dim, dictionary_size).to(device)\n",
    "sae.load_state_dict(state_dict=sae_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<nnsight.intervention.InterventionProxy object at 0x7f5a76198af0>\n",
      "<nnsight.intervention.InterventionProxy object at 0x7f5a76198af0>\n",
      "torch.Size([512])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'tuple' and 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/data/phillip_guo/facts-sae/evaluate_sae.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcais/data/phillip_guo/facts-sae/evaluate_sae.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m test_toks \u001b[39m=\u001b[39m tokenizer(test_batch[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m], return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m, max_length\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m'\u001b[39m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39minput_ids\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bcais/data/phillip_guo/facts-sae/evaluate_sae.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m loss_recovered(test_toks, nn_model, submodule, sae, io\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39min\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/facts-sae/dictionary_learning/evaluation.py:28\u001b[0m, in \u001b[0;36mloss_recovered\u001b[0;34m(tokens, model, submodule, dictionary, io, pct)\u001b[0m\n\u001b[1;32m     25\u001b[0m logits_original \u001b[39m=\u001b[39m invoker\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39mlogits\n\u001b[1;32m     27\u001b[0m \u001b[39m# logits when replacing component output with reconstruction by autoencoder\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[39mwith\u001b[39;00m model\u001b[39m.\u001b[39minvoke(tokens) \u001b[39mas\u001b[39;00m invoker:\n\u001b[1;32m     29\u001b[0m     \u001b[39mif\u001b[39;00m io \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39min\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     30\u001b[0m         submodule\u001b[39m.\u001b[39minput \u001b[39m=\u001b[39m dictionary(submodule\u001b[39m.\u001b[39minput)\n",
      "File \u001b[0;32m~/miniconda3/envs/unlrn/lib/python3.10/site-packages/nnsight/contexts/DirectInvoker.py:32\u001b[0m, in \u001b[0;36mDirectInvoker.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__exit__\u001b[39m(\u001b[39mself\u001b[39m, exc_type, exc_val, exc_tb) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 32\u001b[0m     Invoker\u001b[39m.\u001b[39;49m\u001b[39m__exit__\u001b[39;49m(\u001b[39mself\u001b[39;49m, exc_type, exc_val, exc_tb)\n\u001b[1;32m     33\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfwd_args\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mvalidate\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     34\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfwd_args\n",
      "File \u001b[0;32m~/miniconda3/envs/unlrn/lib/python3.10/site-packages/nnsight/contexts/Invoker.py:76\u001b[0m, in \u001b[0;36mInvoker.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__exit__\u001b[39m(\u001b[39mself\u001b[39m, exc_type, exc_val, exc_tb) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(exc_val, \u001b[39mBaseException\u001b[39;00m):\n\u001b[0;32m---> 76\u001b[0m         \u001b[39mraise\u001b[39;00m exc_val\n",
      "File \u001b[0;32m~/facts-sae/dictionary_learning/evaluation.py:31\u001b[0m, in \u001b[0;36mloss_recovered\u001b[0;34m(tokens, model, submodule, dictionary, io, pct)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mif\u001b[39;00m io \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39min\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     30\u001b[0m     \u001b[39mprint\u001b[39m(submodule\u001b[39m.\u001b[39minput)\n\u001b[0;32m---> 31\u001b[0m     submodule\u001b[39m.\u001b[39minput \u001b[39m=\u001b[39m dictionary(submodule\u001b[39m.\u001b[39;49minput)\n\u001b[1;32m     32\u001b[0m \u001b[39melif\u001b[39;00m io \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mout\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     33\u001b[0m     submodule\u001b[39m.\u001b[39moutput \u001b[39m=\u001b[39m dictionary(submodule\u001b[39m.\u001b[39moutput)\n",
      "File \u001b[0;32m~/miniconda3/envs/unlrn/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/unlrn/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/facts-sae/dictionary_learning/dictionary.py:56\u001b[0m, in \u001b[0;36mAutoEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 56\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode(x))\n",
      "File \u001b[0;32m~/facts-sae/dictionary_learning/dictionary.py:50\u001b[0m, in \u001b[0;36mAutoEncoder.encode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39mprint\u001b[39m(x)\n\u001b[1;32m     49\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> 50\u001b[0m \u001b[39mreturn\u001b[39;00m nn\u001b[39m.\u001b[39mReLU()(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(x \u001b[39m-\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias))\n",
      "File \u001b[0;32m~/miniconda3/envs/unlrn/lib/python3.10/site-packages/nnsight/tracing/Proxy.py:105\u001b[0m, in \u001b[0;36mProxy.__sub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__sub__\u001b[39m(\u001b[39mself\u001b[39m, other: Union[Proxy, Any]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Proxy:\n\u001b[0;32m--> 105\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnode\u001b[39m.\u001b[39;49mgraph\u001b[39m.\u001b[39;49madd(\n\u001b[1;32m    106\u001b[0m         target\u001b[39m=\u001b[39;49moperator\u001b[39m.\u001b[39;49msub,\n\u001b[1;32m    107\u001b[0m         args\u001b[39m=\u001b[39;49m[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnode, other],\n\u001b[1;32m    108\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/unlrn/lib/python3.10/site-packages/nnsight/tracing/Graph.py:208\u001b[0m, in \u001b[0;36mGraph.add\u001b[0;34m(self, target, value, args, kwargs, name)\u001b[0m\n\u001b[1;32m    205\u001b[0m     _args \u001b[39m=\u001b[39m args \u001b[39mif\u001b[39;00m args \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m []\n\u001b[1;32m    206\u001b[0m     _kwargs \u001b[39m=\u001b[39m kwargs \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}\n\u001b[0;32m--> 208\u001b[0m     value \u001b[39m=\u001b[39m target(\n\u001b[1;32m    209\u001b[0m         \u001b[39m*\u001b[39;49mNode\u001b[39m.\u001b[39;49mprepare_proxy_values(_args),\n\u001b[1;32m    210\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mNode\u001b[39m.\u001b[39;49mprepare_proxy_values(_kwargs),\n\u001b[1;32m    211\u001b[0m     )\n\u001b[1;32m    213\u001b[0m target_name \u001b[39m=\u001b[39m target \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(target, \u001b[39mstr\u001b[39m) \u001b[39melse\u001b[39;00m target\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n\u001b[1;32m    215\u001b[0m \u001b[39mif\u001b[39;00m target_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname_idx:\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'tuple' and 'Tensor'"
     ]
    }
   ],
   "source": [
    "test_toks = tokenizer(test_batch['text'], return_tensors='pt', max_length=50, padding='max_length', truncation=True).input_ids\n",
    "loss_recovered(test_toks, nn_model, submodule, sae, io='in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37590272 allocated, 0.38797312 reserved, 84.986691584 total\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "def display_memory():\n",
    "    total = torch.cuda.get_device_properties(0).total_memory\n",
    "    r = torch.cuda.memory_reserved(0)\n",
    "    a = torch.cuda.memory_allocated(0)\n",
    "    print(f\"{a*1e-9} allocated, {r*1e-9} reserved, {total*1e-9} total\")\n",
    "display_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlrn-2",
   "language": "python",
   "name": "unlrn-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
